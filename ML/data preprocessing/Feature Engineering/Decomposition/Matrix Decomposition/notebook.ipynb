{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Decomposition\n",
    "\n",
    "A matrix decomposition is a way of reducing a matrix into its constituent parts. It is an approach that can simplify more complex matrix operations that can be performed on the decomposed matrix rather than on the original matrix itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary Learning\n",
    "\n",
    "Finds a dictionary (a set of atoms) that performs well at sparsely encoding the fitted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_sparse_coded_signal\n",
    "from sklearn.decomposition import DictionaryLearning\n",
    "X, dictionary, code = make_sparse_coded_signal(\n",
    "    n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\n",
    "    random_state=42, data_transposed=False\n",
    ")\n",
    "dict_learner = DictionaryLearning(\n",
    "    n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\n",
    "    random_state=42,\n",
    ")\n",
    "X_transformed = dict_learner.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can check the level of sparsity of X_transformed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41733333333333333"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_transformed == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the average squared euclidean norm of the reconstruction error of the sparse coded signal relative to the squared euclidean norm of the original signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07777084613290733"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hat = X_transformed @ dict_learner.components_\n",
    "np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Analysis\n",
    "\n",
    "FactorAnalysis performs a maximum likelihood estimate of the so-called loading matrix, the transformation of the latent variables to the observed ones, using SVD based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "X, _ = load_digits(return_X_y=True)\n",
    "transformer = FactorAnalysis(n_components=7, random_state=0)\n",
    "X_transformed = transformer.fit_transform(X)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastICA\n",
    "\n",
    "A fast algorithm for Independent Component Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:120: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1797, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import FastICA\n",
    "X, _ = load_digits(return_X_y=True)\n",
    "transformer = FastICA(n_components=7,\n",
    "        random_state=0,\n",
    "        whiten='unit-variance')\n",
    "X_transformed = transformer.fit_transform(X)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental PCA\n",
    "\n",
    "Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA, and allows sparse input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from scipy import sparse\n",
    "X, _ = load_digits(return_X_y=True)\n",
    "transformer = IncrementalPCA(n_components=7, batch_size=200)\n",
    "# either partially fit on smaller batches of data\n",
    "transformer.partial_fit(X[:100, :])\n",
    "\n",
    "# or let the fit function itself divide the data into batches\n",
    "X_sparse = sparse.csr_matrix(X)\n",
    "X_transformed = transformer.fit_transform(X_sparse)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.38340578, -0.2935787 ],\n",
       "       [-2.22189802,  0.25133484],\n",
       "       [-3.6053038 , -0.04224385],\n",
       "       [ 1.38340578,  0.2935787 ],\n",
       "       [ 2.22189802, -0.25133484],\n",
       "       [ 3.6053038 ,  0.04224385]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2],\n",
    "              [1, 1], [2, 1], [3, 2]])\n",
    "ipca = IncrementalPCA(n_components=2, batch_size=3)\n",
    "ipca.fit(X)\n",
    "ipca.transform(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA\n",
    "\n",
    "Non-linear dimensionality reduction through the use of kernels (see Pairwise metrics, Affinities and Kernels).\n",
    "\n",
    "kernels ==> ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘cosine’, ‘precomputed’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 7)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import KernelPCA\n",
    "X, _ = load_digits(return_X_y=True)\n",
    "transformer = KernelPCA(n_components=7, kernel='linear')\n",
    "X_transformed = transformer.fit_transform(X)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "Latent Dirichlet Allocation with online variational Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],\n",
       "       [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "# This produces a feature matrix of token counts, similar to what\n",
    "# CountVectorizer would produce on text.\n",
    "X, _ = make_multilabel_classification(random_state=0)\n",
    "lda = LatentDirichletAllocation(n_components=5,\n",
    "    random_state=0)\n",
    "lda.fit(X)\n",
    "# get topics for some given samples:\n",
    "lda.transform(X[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Batch Dictionary Learning\n",
    "\n",
    "Finds a dictionary (a set of atoms) that performs well at sparsely encoding the fitted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_sparse_coded_signal\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "X, dictionary, code = make_sparse_coded_signal(\n",
    "    n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\n",
    "    random_state=42, data_transposed=False)\n",
    "dict_learner = MiniBatchDictionaryLearning(\n",
    "    n_components=15, batch_size=3, transform_algorithm='lasso_lars',\n",
    "    transform_alpha=0.1, random_state=42)\n",
    "X_transformed = dict_learner.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38466666666666666"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_transformed == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0597265626470165"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hat = X_transformed @ dict_learner.components_\n",
    "np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Batch Sparse PCA\n",
    "\n",
    "Finds the set of sparse components that can optimally reconstruct the data. The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.decomposition import MiniBatchSparsePCA\n",
    "X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n",
    "transformer = MiniBatchSparsePCA(n_components=5, batch_size=50, random_state=0)\n",
    "transformer.fit(X)\n",
    "\n",
    "X_transformed = transformer.transform(X)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most values in the components_ are zero (sparsity)\n",
    "np.mean(transformer.components_ == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF (Non-Negative Matrix Factorization)\n",
    "\n",
    "Find two non-negative matrices, i.e. matrices with all non-negative elements, (W, H) whose product approximates the non-negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.09783018, 0.30560234],\n",
       "       [2.13443044, 2.13171694]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=2, init='random', random_state=0)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Batch NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devangm/.local/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:2308: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.36064875, 0.13259228],\n",
       "       [1.4397403 , 1.38388453]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n",
    "from sklearn.decomposition import MiniBatchNMF\n",
    "model = MiniBatchNMF(n_components=2, init='random', random_state=0)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99244289 0.00755711]\n",
      "[6.30061232 0.54980396]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99244289 0.00755711]\n",
      "[6.30061232 0.54980396]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "pca.fit(X)\n",
    "PCA(n_components=2, svd_solver='full')\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99244289]\n",
      "[6.30061232]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=1, svd_solver='arpack')\n",
    "pca.fit(X)\n",
    "PCA(n_components=1, svd_solver='arpack')\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse PCA\n",
    "\n",
    "Finds the set of sparse components that can optimally reconstruct the data. The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.decomposition import SparsePCA\n",
    "X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n",
    "transformer = SparsePCA(n_components=5, random_state=0)\n",
    "transformer.fit(X)\n",
    "X_transformed = transformer.transform(X)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most values in the components_ are zero (sparsity)\n",
    "np.mean(transformer.components_ == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Coder\n",
    "\n",
    "Finds a sparse representation of data against a fixed, precomputed dictionary.\n",
    "\n",
    "Each row of the result is the solution to a sparse coding problem. The goal is to find a sparse array code such that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., -1.,  0.,  0.],\n",
       "       [ 0.,  1.,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import SparseCoder\n",
    "X = np.array([[-1, -1, -1], [0, 0, 3]])\n",
    "dictionary = np.array(\n",
    "    [[0, 1, 0],\n",
    "    [-1, -1, 2],\n",
    "    [1, 1, 1],\n",
    "    [0, 1, 1],\n",
    "    [0, 2, 1]],\n",
    "  dtype=np.float64\n",
    ")\n",
    "coder = SparseCoder(\n",
    "    dictionary=dictionary, transform_algorithm='lasso_lars',\n",
    "    transform_alpha=1e-10,\n",
    ")\n",
    "coder.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated SVD\n",
    "\n",
    "Dimensionality reduction using truncated SVD (aka LSA).\n",
    "\n",
    "This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with sparse matrices efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01570766 0.05122679 0.04998062 0.04795064 0.04539933]\n",
      "0.2102650346507035\n",
      "[35.24105443  4.5981613   4.54200434  4.44866153  4.32887456]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X_dense = np.random.rand(100, 100)\n",
    "X_dense[:, 2 * np.arange(50)] = 0\n",
    "X = csr_matrix(X_dense)\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "svd.fit(X)\n",
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "print(svd.singular_values_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
