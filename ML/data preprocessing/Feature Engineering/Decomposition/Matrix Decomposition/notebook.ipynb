{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Decomposition\n",
    "\n",
    "A matrix decomposition is a way of reducing a matrix into its constituent parts. It is an approach that can simplify more complex matrix operations that can be performed on the decomposed matrix rather than on the original matrix itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary Learning\n",
    "\n",
    "Finds a dictionary (a set of atoms) that performs well at sparsely encoding the fitted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_sparse_coded_signal\n",
    "from sklearn.decomposition import DictionaryLearning\n",
    "X, dictionary, code = make_sparse_coded_signal(\n",
    "    n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\n",
    "    random_state=42, data_transposed=False\n",
    ")\n",
    "dict_learner = DictionaryLearning(\n",
    "    n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\n",
    "    random_state=42,\n",
    ")\n",
    "X_transformed = dict_learner.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can check the level of sparsity of X_transformed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41733333333333333"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_transformed == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the average squared euclidean norm of the reconstruction error of the sparse coded signal relative to the squared euclidean norm of the original signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07777084613290733"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hat = X_transformed @ dict_learner.components_\n",
    "np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Analysis\n",
    "\n",
    "FactorAnalysis performs a maximum likelihood estimate of the so-called loading matrix, the transformation of the latent variables to the observed ones, using SVD based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "X, _ = load_digits(return_X_y=True)\n",
    "transformer = FactorAnalysis(n_components=7, random_state=0)\n",
    "X_transformed = transformer.fit_transform(X)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastICA\n",
    "\n",
    "A fast algorithm for Independent Component Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:120: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1797, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import FastICA\n",
    "X, _ = load_digits(return_X_y=True)\n",
    "transformer = FastICA(n_components=7,\n",
    "        random_state=0,\n",
    "        whiten='unit-variance')\n",
    "X_transformed = transformer.fit_transform(X)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental PCA\n",
    "\n",
    "Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA, and allows sparse input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from scipy import sparse\n",
    "X, _ = load_digits(return_X_y=True)\n",
    "transformer = IncrementalPCA(n_components=7, batch_size=200)\n",
    "# either partially fit on smaller batches of data\n",
    "transformer.partial_fit(X[:100, :])\n",
    "\n",
    "# or let the fit function itself divide the data into batches\n",
    "X_sparse = sparse.csr_matrix(X)\n",
    "X_transformed = transformer.fit_transform(X_sparse)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.38340578, -0.2935787 ],\n",
       "       [-2.22189802,  0.25133484],\n",
       "       [-3.6053038 , -0.04224385],\n",
       "       [ 1.38340578,  0.2935787 ],\n",
       "       [ 2.22189802, -0.25133484],\n",
       "       [ 3.6053038 ,  0.04224385]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2],\n",
    "              [1, 1], [2, 1], [3, 2]])\n",
    "ipca = IncrementalPCA(n_components=2, batch_size=3)\n",
    "ipca.fit(X)\n",
    "ipca.transform(X) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
