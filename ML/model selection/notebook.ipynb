{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "Model selection is the process of selecting one final machine learning model from among a collection of candidate machine learning models for a training dataset. Model selection is a process that can be applied both across different types of models (e.g. logistic regression, SVM, KNN, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFlod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "kf = KFold(n_splits=2)\n",
    "kf.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=2, random_state=None, shuffle=False)\n",
      "TRAIN: [2 3] TEST: [0 1]\n",
      "TRAIN: [0 1] TEST: [2 3]\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupKFold\n",
    "\n",
    "K-Fold variant with non-overlapping groups.\n",
    "\n",
    "Same group will not appear in two different folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "groups = np.array([0, 0, 2, 2])\n",
    "group_kfold = GroupKFold(n_splits=2)\n",
    "group_kfold.get_n_splits(X, y, groups)\n",
    "2\n",
    "print(group_kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in group_kfold.split(X, y, groups):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated Stratified K Fold\n",
    "\n",
    "Repeats Stratified K-Fold n times with different randomization in each repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [1 2] TEST: [0 3]\n",
      "TRAIN: [0 3] TEST: [1 2]\n",
      "TRAIN: [1 3] TEST: [0 2]\n",
      "TRAIN: [0 2] TEST: [1 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,\n",
    "    random_state=36851234)\n",
    "for train_index, test_index in rskf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Group KFold\n",
    "\n",
    "Stratified K-Folds iterator variant with non-overlapping groups.\n",
    "\n",
    "This cross-validation object is a variation of StratifiedKFold attempts to return stratified folds with non-overlapping groups. The folds are made by preserving the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [1 1 2 2 4 5 5 5 5 8 8]\n",
      "       [0 0 1 1 1 0 0 0 0 0 0]\n",
      " TEST: [3 3 3 6 6 7]\n",
      "       [1 1 1 0 0 0]\n",
      "TRAIN: [3 3 3 4 5 5 5 5 6 6 7]\n",
      "       [1 1 1 1 0 0 0 0 0 0 0]\n",
      " TEST: [1 1 2 2 8 8]\n",
      "       [0 0 1 1 0 0]\n",
      "TRAIN: [1 1 2 2 3 3 3 6 6 7 8 8]\n",
      "       [0 0 1 1 1 1 1 0 0 0 0 0]\n",
      " TEST: [4 5 5 5 5]\n",
      "       [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "X = np.ones((17, 2))\n",
    "y = np.array([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "groups = np.array([1, 1, 2, 2, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 7, 8, 8])\n",
    "cv = StratifiedGroupKFold(n_splits=3)\n",
    "for train_idxs, test_idxs in cv.split(X, y, groups):\n",
    "    print(\"TRAIN:\", groups[train_idxs])\n",
    "    print(\"      \", y[train_idxs])\n",
    "    print(\" TEST:\", groups[test_idxs])\n",
    "    print(\"      \", y[test_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Split\n",
    "\n",
    "Random permutation cross-validator\n",
    "\n",
    "Yields indices to split data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])\n",
    "y = np.array([1, 2, 1, 2, 1, 2])\n",
    "rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n",
    "rs.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)\n",
      "TRAIN: [1 3 0 4] TEST: [5 2]\n",
      "TRAIN: [4 0 2 5] TEST: [1 3]\n",
      "TRAIN: [1 2 4 0] TEST: [3 5]\n",
      "TRAIN: [3 4 1 0] TEST: [5 2]\n",
      "TRAIN: [3 5 1 0] TEST: [2 4]\n"
     ]
    }
   ],
   "source": [
    "print(rs)\n",
    "for train_index, test_index in rs.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [1 3 0] TEST: [5 2]\n",
      "TRAIN: [4 0 2] TEST: [1 3]\n",
      "TRAIN: [1 2 4] TEST: [3 5]\n",
      "TRAIN: [3 4 1] TEST: [5 2]\n",
      "TRAIN: [3 5 1] TEST: [2 4]\n"
     ]
    }
   ],
   "source": [
    "rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,\n",
    "                  random_state=0)\n",
    "for train_index, test_index in rs.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Shuffle Split\n",
    "\n",
    "Stratified ShuffleSplit cross-validator\n",
    "\n",
    "Provides train/test indices to split data in train/test sets.\n",
    "\n",
    "This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "sss.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=5, random_state=0, test_size=0.5,\n",
      "            train_size=None)\n",
      "TRAIN: [5 2 3] TEST: [4 1 0]\n",
      "TRAIN: [5 1 4] TEST: [0 2 3]\n",
      "TRAIN: [5 0 2] TEST: [4 3 1]\n",
      "TRAIN: [4 1 0] TEST: [2 3 5]\n",
      "TRAIN: [0 5 1] TEST: [3 4 2]\n"
     ]
    }
   ],
   "source": [
    "print(sss)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Shuffle Split\n",
    "\n",
    "Provides randomized train/test indices to split data according to a third-party provided group. This group information can be used to encode arbitrary domain specific stratifications of the samples as integers.\n",
    "\n",
    "The difference between LeavePGroupsOut and GroupShuffleSplit is that the former generates splits using all subsets of size p unique groups, whereas GroupShuffleSplit generates a user-determined number of random test splits, each with a user-determined fraction of unique groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "X = np.ones(shape=(8, 2))\n",
    "y = np.ones(shape=(8, 1))\n",
    "groups = np.array([1, 1, 2, 2, 2, 3, 3, 3])\n",
    "print(groups.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(n_splits=2, train_size=.7, random_state=42)\n",
    "gss.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [2 3 4 5 6 7] TEST: [0 1]\n",
      "TRAIN: [0 1 5 6 7] TEST: [2 3 4]\n"
     ]
    }
   ],
   "source": [
    "for train_idx, test_idx in gss.split(X, y, groups):\n",
    "    print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Group P Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 1, 2])\n",
    "groups = np.array([1, 1, 2, 2])\n",
    "logo = LeaveOneGroupOut()\n",
    "logo.get_n_splits(X, y, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logo.get_n_splits(groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeaveOneGroupOut()\n",
      "TRAIN: [2 3] TEST: [0 1]\n",
      "[[5 6]\n",
      " [7 8]] [[1 2]\n",
      " [3 4]] [1 2] [1 2]\n",
      "TRAIN: [0 1] TEST: [2 3]\n",
      "[[1 2]\n",
      " [3 4]] [[5 6]\n",
      " [7 8]] [1 2] [1 2]\n"
     ]
    }
   ],
   "source": [
    "print(logo)\n",
    "for train_index, test_index in logo.split(X, y, groups):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave P Groups Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([1, 2, 1])\n",
    "groups = np.array([1, 2, 3])\n",
    "lpgo = LeavePGroupsOut(n_groups=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpgo.get_n_splits(X, y, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpgo.get_n_splits(groups=groups)  # 'groups' is always required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeavePGroupsOut(n_groups=2)\n"
     ]
    }
   ],
   "source": [
    "print(lpgo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [2] TEST: [0 1]\n",
      "[[5 6]] [[1 2]\n",
      " [3 4]] [1] [1 2]\n",
      "TRAIN: [1] TEST: [0 2]\n",
      "[[3 4]] [[1 2]\n",
      " [5 6]] [2] [1 1]\n",
      "TRAIN: [0] TEST: [1 2]\n",
      "[[1 2]] [[3 4]\n",
      " [5 6]] [1] [2 1]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in lpgo.split(X, y, groups):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "y = np.array([1, 2])\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeaveOneOut()\n",
      "TRAIN: [1] TEST: [0]\n",
      "[[3 4]] [[1 2]] [2] [1]\n",
      "TRAIN: [0] TEST: [1]\n",
      "[[1 2]] [[3 4]] [1] [2]\n"
     ]
    }
   ],
   "source": [
    "print(loo)\n",
    "for train_index, test_index in loo.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave P Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeavePOut\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "lpo = LeavePOut(2)\n",
    "lpo.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeavePOut(p=2)\n",
      "TRAIN: [2 3] TEST: [0 1]\n",
      "TRAIN: [1 3] TEST: [0 2]\n",
      "TRAIN: [1 2] TEST: [0 3]\n",
      "TRAIN: [0 3] TEST: [1 2]\n",
      "TRAIN: [0 2] TEST: [1 3]\n",
      "TRAIN: [0 1] TEST: [2 3]\n"
     ]
    }
   ],
   "source": [
    "print(lpo)\n",
    "for train_index, test_index in lpo.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "test_fold = [0, 1, -1, 1]\n",
    "ps = PredefinedSplit(test_fold)\n",
    "ps.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredefinedSplit(test_fold=array([ 0,  1, -1,  1]))\n",
      "TRAIN: [1 2 3] TEST: [0]\n",
      "TRAIN: [0 2] TEST: [1 3]\n"
     ]
    }
   ],
   "source": [
    "print(ps)\n",
    "for train_index, test_index in ps.split():\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeSeries Split\n",
    "\n",
    "Time Series cross-validator\n",
    "\n",
    "Provides train/test indices to split time series data samples that are observed at fixed time intervals, in train/test sets. In each split, test indices must be higher than before, and thus shuffling in cross validator is inappropriate.\n",
    "\n",
    "This cross-validation object is a variation of KFold. In the kth split, it returns first k folds as train set and the (k+1)th fold as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "tscv = TimeSeriesSplit()\n",
    "print(tscv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [0] TEST: [1]\n",
      "TRAIN: [0 1] TEST: [2]\n",
      "TRAIN: [0 1 2] TEST: [3]\n",
      "TRAIN: [0 1 2 3] TEST: [4]\n",
      "TRAIN: [0 1 2 3 4] TEST: [5]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in tscv.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [0 1 2 3 4 5] TEST: [6 7]\n",
      "TRAIN: [0 1 2 3 4 5 6 7] TEST: [8 9]\n",
      "TRAIN: [0 1 2 3 4 5 6 7 8 9] TEST: [10 11]\n"
     ]
    }
   ],
   "source": [
    "# Fix test_size to 2 with 12 samples\n",
    "X = np.random.randn(12, 2)\n",
    "y = np.random.randint(0, 2, 12)\n",
    "tscv = TimeSeriesSplit(n_splits=3, test_size=2)\n",
    "for train_index, test_index in tscv.split(X):\n",
    "  print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [0 1 2 3] TEST: [6 7]\n",
      "TRAIN: [0 1 2 3 4 5] TEST: [8 9]\n",
      "TRAIN: [0 1 2 3 4 5 6 7] TEST: [10 11]\n"
     ]
    }
   ],
   "source": [
    "# Add in a 2 period gap\n",
    "tscv = TimeSeriesSplit(n_splits=3, test_size=2, gap=2)\n",
    "for train_index, test_index in tscv.split(X):\n",
    "  print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Havling Grid Search CV\n",
    "\n",
    "Search over specified parameter values with successive halving.\n",
    "\n",
    "The search strategy starts evaluating all the candidates with a small amount of resources and iteratively selects the best candidates, using more and more resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None, 'min_samples_split': 5, 'n_estimators': 9}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"min_samples_split\": [5, 10]}\n",
    "search = HalvingGridSearchCV(clf, param_grid, resource='n_estimators',\n",
    "max_resources=10,\n",
    "random_state=0).fit(X, y)\n",
    "search.best_params_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Grid\n",
    "\n",
    "Grid of parameters with a discrete number of values for each.\n",
    "\n",
    "Can be used to iterate over parameter value combinations with the Python built-in function iter. The order of the generated parameter combinations is deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "param_grid = {'a': [1, 2], 'b': [True, False]}\n",
    "list(ParameterGrid(param_grid)) == (\n",
    "   [{'a': 1, 'b': True}, {'a': 1, 'b': False},\n",
    "    {'a': 2, 'b': True}, {'a': 2, 'b': False}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]\n",
    "list(ParameterGrid(grid)) == [{'kernel': 'linear'},\n",
    "                              {'kernel': 'rbf', 'gamma': 1},\n",
    "                              {'kernel': 'rbf', 'gamma': 10}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ParameterGrid(grid)[1] == {'kernel': 'rbf', 'gamma': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Sampler\n",
    "\n",
    "Generator on parameters sampled from given distributions.\n",
    "\n",
    "Non-deterministic iterable over random candidate combinations for hyper- parameter search. If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy.stats.distributions import expon\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(0)\n",
    "param_grid = {'a':[1, 2], 'b': expon()}\n",
    "param_list = list(ParameterSampler(param_grid, n_iter=4, random_state=rng))\n",
    "rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items())\n",
    "                for d in param_list]\n",
    "rounded_list == [{'b': 0.89856, 'a': 1},\n",
    "  {'b': 0.923223, 'a': 1},\n",
    "  {'b': 1.878964, 'a': 2},\n",
    "  {'b': 1.038159, 'a': 2}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search CV\n",
    "\n",
    "Randomized search on hyper parameters.\n",
    "\n",
    "RandomizedSearchCV implements a “fit” and a “score” method. It also implements “score_samples”, “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2.195254015709299, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "iris = load_iris()\n",
    "logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
    "                              random_state=0)\n",
    "distributions = dict(C=uniform(loc=0, scale=4),\n",
    "                  penalty=['l2', 'l1'])\n",
    "clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "search = clf.fit(iris.data, iris.target)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Having Radom Search CV\n",
    "\n",
    "Randomized search on hyper parameters.\n",
    "\n",
    "The search strategy starts evaluating all the candidates with a small amount of resources and iteratively selects the best candidates, using more and more resources.\n",
    "\n",
    "The candidates are sampled at random from the parameter space and the number of sampled candidates is determined by n_candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3, 'min_samples_split': 3, 'n_estimators': 9}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "np.random.seed(0)\n",
    "param_distributions = {\"max_depth\": [3, None],\n",
    "                      \"min_samples_split\": randint(2, 11)}\n",
    "search = HalvingRandomSearchCV(clf, param_distributions,\n",
    "resource='n_estimators',\n",
    "max_resources=10,\n",
    "random_state=0).fit(X, y)\n",
    "search.best_params_  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
